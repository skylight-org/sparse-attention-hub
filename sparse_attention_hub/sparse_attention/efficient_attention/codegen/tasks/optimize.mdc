---
alwaysApply: false
---
backend=native
Backend=Native
baseline=pqcache
Baseline=PQCache
function_name=next
workspace_dir=implementations/{baseline}/workspace/
iteration_directory: workspace/iter-{iteration}

run the following to active correct conda environment
```
source /workspace/anaconda3/bin/activate && conda activate sparse_attention_hub 
```


You are supposed to write an optimized cuda code for the function in {workspace_dir}/gen_imperative_code.py named __indexer_{function_name}() which extracts the core logic of the sparse attention config defined in implementations/{baseline}/research.py. You can identify parts of the code that should be written as a triton cuda kernel and used inside this function, including cases where the entire function can be written as a cuda kernel. 

The final function corresponding to the __indexer_{function_name}() should be written as a function in __indexer_{function_name}() in file with name {iteration_directory}/optimized_indexer.py (where iteration = 1,2,3..) Follow the following 


For each iteration, create a plan you want to implement for improvement over the given code / previous iteration. Once implemented, ensure that the code is correct using. If incorrect, you should keep fixing issues till you get the correct code.

```
CUDA_VISIBLE_DEVICES=0 python -m sparse_attention_hub.sparse_attention.efficient_attention.codegen.correctness \
  --class1 sparse_attention_hub.sparse_attention.efficient_attention.implementations.{baseline}.research.{Baseline}ResearchBackend \
  --class2 sparse_attention_hub.sparse_attention.efficient_attention.implementations.{baseline}.{backend}.{Baseline}{Backend}Backend \
  --function indexer_{function_name} --indexer-{function_name}-file {iteration_dir}/optimized_indexer.py
```

Once correct optimized code implementing the plan is obtained, then you should profile the code using
```
CUDA_VISIBLE_DEVICES=0 python -m sparse_attention_hub.sparse_attention.efficient_attention.codegen.profile \
  --class1 sparse_attention_hub.sparse_attention.efficient_attention.implementations.{baseline}.research.{baseline}ResearchBackend \
  --class2 sparse_attention_hub.sparse_attention.efficient_attention.implementations.{baseline}.{backend}.{baseline}{Backend}Backend \
  --function indexer_{funtion_name} --indexer-{function_name}-file  {iteration_directory}/optimized_indexer.py &> {iteration_directory}/profile.log 
```

Use the dumped {iteration_directory}/profile* files to make a concise summary file in {iteration_directory}/summary.md which should note 
1. what idea was tried
2. what are the timing results
3. what do the trace files and top 10 operations in profile.log say and what are more opportunities for optimization. 

Then using the insights from current trace begin the next iteration.

If iterations already exist in {iteration_directory}/ then continue from the last iteration after reading all the previous tries at {workspace_dir}/{iterations}/summary.md