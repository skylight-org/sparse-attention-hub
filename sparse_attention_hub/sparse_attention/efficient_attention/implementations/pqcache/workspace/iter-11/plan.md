# Iteration 11: Eliminate torch.arange with Custom Triton Kernel

## üéØ Goal

Replace all `torch.arange` calls (which cause scattered kernel launches) with a single custom Triton kernel that generates all indices and weights in one pass.

## üîç Problem Analysis

### Current torch.arange Usage (Iter-9)

```python
# 4-5 torch.arange calls causing separate kernel launches:

# 1. Sink indices (line 247)
sparse_list[:, :, :actual_sink_size] = torch.arange(actual_sink_size, ...).view(1, 1, -1)

# 2. Window indices (line 250)  
sparse_list[:, :, end:] = torch.arange(window_start, sk, ...).view(1, 1, -1)

# 3. Batch indices (line 259)
batch_indices = torch.arange(b, ...).view(b, 1, 1)

# 4. Head indices (line 260)
head_indices = torch.arange(h, ...).view(1, h, 1)

# 5. Advanced indexing (line 261) - causes many internal launches
weight_list[batch_indices, head_indices, sparse_list] = 1.0
```

**Each torch.arange**:
- Launches a separate CUDA kernel (~18 Œºs each)
- Has Python dispatch overhead (~10 Œºs)
- Creates temporary tensors (allocation overhead)

**Total cost**: ~145 Œºs from profile data!

## üöÄ Solution: Single Triton Kernel

Write one kernel that does EVERYTHING:
1. Generate sink indices: [0, 1, ..., sink_size-1]
2. Copy heavy hitter indices from topk
3. Generate window indices: [sk-window_size, ..., sk-1]
4. Set weights to 1.0 for all attended positions

### Kernel Design

```python
@triton.jit
def generate_sparse_indices_and_weights_kernel(
    # Outputs
    sparse_list_ptr,     # [b, h, total_attended]
    weight_list_ptr,     # [b, h, sk]
    # Inputs
    topk_indices_ptr,    # [b, h, heavy_size]
    # Dimensions
    b: tl.constexpr,
    h: tl.constexpr,
    sk,
    sink_size,
    heavy_size,
    window_size,
    init_offset,
    # Strides
    ...
):
    """Generate ALL indices and weights in a single kernel pass."""
    
    # Each program handles one (batch, head) pair
    pid = tl.program_id(0)
    batch_idx = pid // h
    head_idx = pid % h
    
    # Calculate base pointers
    sparse_base = sparse_list_ptr + batch_idx * sparse_stride_b + head_idx * sparse_stride_h
    weight_base = weight_list_ptr + batch_idx * weight_stride_b + head_idx * weight_stride_h
    topk_base = topk_indices_ptr + batch_idx * topk_stride_b + head_idx * topk_stride_h
    
    total_attended = sink_size + heavy_size + window_size
    
    # Process in blocks
    BLOCK_SIZE: tl.constexpr = 256
    
    for block_start in range(0, total_attended, BLOCK_SIZE):
        offsets = block_start + tl.arange(0, BLOCK_SIZE)
        mask = offsets < total_attended
        
        # Determine which region each offset belongs to
        in_sink = offsets < sink_size
        in_heavy = (offsets >= sink_size) & (offsets < sink_size + heavy_size)
        in_window = offsets >= sink_size + heavy_size
        
        # Generate index values
        # Sink: just the offset itself
        sink_vals = offsets
        
        # Heavy: load from topk_indices and add init_offset
        heavy_offset = offsets - sink_size
        heavy_vals = tl.load(topk_base + heavy_offset, mask=in_heavy, other=0) + init_offset
        
        # Window: window_start + (offset - sink_size - heavy_size)
        window_start = sk - window_size
        window_vals = window_start + (offsets - sink_size - heavy_size)
        
        # Select the right value based on region
        sparse_vals = tl.where(in_sink, sink_vals,
                      tl.where(in_heavy, heavy_vals, window_vals))
        
        # Store to sparse_list
        tl.store(sparse_base + offsets, sparse_vals, mask=mask)
        
        # Set weights to 1.0 at these indices
        # This is a scatter operation: weight_list[sparse_vals] = 1.0
        for i in range(BLOCK_SIZE):
            if block_start + i < total_attended:
                idx = tl.load(sparse_base + block_start + i)
                tl.store(weight_base + idx, 1.0)
```

### Launch Parameters

```python
# One thread block per (batch, head) pair
grid = (b * h,)

generate_sparse_indices_and_weights_kernel[grid](
    sparse_list, weight_list, topk_indices,
    b, h, sk, sink_size, heavy_size, window_size, init_offset,
    ...strides...
)
```

## üìä Expected Performance

### Kernel Count Reduction

```
Current (iter-9): ~29 kernel launches
‚îú‚îÄ pq_score_kernel_v6:  1 launch
‚îú‚îÄ topk internals:     15 launches
‚îú‚îÄ torch.arange:        4 launches ‚Üê ELIMINATE!
‚îú‚îÄ advanced indexing:   8 launches ‚Üê ELIMINATE!
‚îî‚îÄ misc:                1 launch

After (iter-11): ~17 kernel launches (-41%)
‚îú‚îÄ pq_score_kernel_v6:         1 launch
‚îú‚îÄ topk internals:            15 launches
‚îú‚îÄ generate_indices_weights:   1 launch ‚Üê NEW!
‚îî‚îÄ (torch.arange eliminated)

Saved: 12 kernel launches!
```

### Time Breakdown

```
Current overhead from torch.arange + indexing:
‚îú‚îÄ torch.arange:        ~72 Œºs (4 calls √ó 18 Œºs)
‚îú‚îÄ Advanced indexing:   ~159 Œºs (weight_list[...] = 1.0)
‚îú‚îÄ Launch overhead:     ~30 Œºs (12 launches √ó 2.5 Œºs)
‚îî‚îÄ Total:               ~261 Œºs

After custom kernel:
‚îú‚îÄ generate_indices:    ~30-40 Œºs (single optimized kernel)
‚îú‚îÄ Launch overhead:     ~3 Œºs (1 launch)
‚îî‚îÄ Total:               ~33-43 Œºs

Expected gain: ~218-228 Œºs (82% reduction in this component!)
```

### Overall Performance

```
Iter-9:    465 Œºs
‚îú‚îÄ CUDA:   171 Œºs
‚îî‚îÄ Overhead: 294 Œºs
    ‚îú‚îÄ arange + indexing: 261 Œºs ‚Üê TARGET
    ‚îî‚îÄ other: 33 Œºs

Iter-11:   ~250-270 Œºs (estimated)
‚îú‚îÄ CUDA:   171 + 40 = 211 Œºs (PQ + indices kernel)
‚îî‚îÄ Overhead: ~40-60 Œºs (minimal!)

Expected improvement: ~195-215 Œºs (42-46% faster!)
```

## üéì Advantages

### 1. Eliminate All torch.arange Calls
- No Python dispatch overhead
- No temporary tensor allocations
- No separate kernel launches

### 2. Fused Weight Assignment
- weight_list[...] = 1.0 done in same kernel
- No advanced indexing overhead
- Direct scatter operation

### 3. Single Kernel Launch
- Minimal overhead (~3 Œºs vs ~30 Œºs)
- Better GPU utilization
- Predictable performance

### 4. Optimized Memory Access
- Coalesced writes to sparse_list
- Efficient scatter to weight_list
- No intermediate buffers

## ‚ö†Ô∏è Challenges

### 1. Scatter Operation

```python
# This is tricky in Triton:
weight_list[sparse_vals[i]] = 1.0

# Need to do it carefully to avoid race conditions
# Multiple threads might write to same location (OK since all write 1.0)
```

**Solution**: Use atomic operations or accept races (writing 1.0 is idempotent)

### 2. Variable-length Regions

```python
# sink_size, heavy_size, window_size can vary
# Need to handle different sizes efficiently
```

**Solution**: Use masks and conditional logic in kernel

### 3. Memory Ordering

```python
# Must initialize weight_list to zeros first
# Then scatter 1.0s

weight_list = torch.zeros((b, h, sk), ...)  # Still need this
```

**Alternative**: Have kernel do both initialization AND scatter (slower but cleaner)

## üìù Implementation Steps

1. Write the Triton kernel
2. Test with simple cases (fixed sizes)
3. Add dynamic size handling
4. Optimize memory access patterns
5. Profile and compare

## ‚úÖ Success Criteria

1. ‚úÖ Eliminate all torch.arange calls
2. ‚úÖ Eliminate advanced indexing overhead
3. ‚úÖ Reduce kernel launches by 12
4. ‚úÖ Improve wall-clock time by 180-220 Œºs
5. ‚úÖ Pass correctness tests
6. ‚úÖ Lower variance than iter-9

## üéØ Realistic Expectation

**Conservative estimate**: 280-300 Œºs (40% faster than iter-9)
**Optimistic estimate**: 250-270 Œºs (45% faster than iter-9)

This would finally bring us close to the theoretical minimum!

